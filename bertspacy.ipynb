{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef\n",
    "\n",
    "WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "RDFS = rdflib.namespace.RDFS\n",
    "SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "\n",
    "class NamespaceWrapper:\n",
    "    WD = WD\n",
    "    WDT = WDT\n",
    "    DDIS = DDIS\n",
    "    RDFS = RDFS\n",
    "    SCHEMA = SCHEMA\n",
    "\n",
    "    @staticmethod\n",
    "    def uri_to_prefixed(uri):\n",
    "        uri_ref = URIRef(uri)\n",
    "        if str(uri_ref).startswith(str(WD)):\n",
    "            return getattr(NamespaceWrapper, \"WD\")[str(uri_ref).replace(str(WD), '')]\n",
    "        elif str(uri_ref).startswith(str(WDT)):\n",
    "            return getattr(NamespaceWrapper, \"WDT\")[str(uri_ref).replace(str(WDT), '')]\n",
    "        elif str(uri_ref).startswith(str(DDIS)):\n",
    "            return getattr(NamespaceWrapper, \"DDIS\")[str(uri_ref).replace(str(DDIS), '')]\n",
    "        elif str(uri_ref).startswith(str(RDFS)):\n",
    "            return getattr(NamespaceWrapper, \"RDFS\")[str(uri_ref).replace(str(RDFS), '')]\n",
    "        elif str(uri_ref).startswith(str(SCHEMA)):\n",
    "            return getattr(NamespaceWrapper, \"SCHEMA\")[str(uri_ref).replace(str(SCHEMA), '')]\n",
    "        else:\n",
    "            return uri_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded key_embeddings from file.\n",
      "Loaded key_embeddings from file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import rdflib\n",
    "from rdflib import URIRef\n",
    "\n",
    "# Define your dictionary\n",
    "graph = rdflib.Graph()\n",
    "graph.parse(r'C:\\Users\\dli0305\\Downloads\\ddis-movie-graph.nt\\14_graph.nt', format='turtle')\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "entity_emb = np.load(r'C:\\Users\\dli0305\\Downloads\\ddis-graph-embeddings\\ddis-graph-embeddings\\entity_embeds.npy')\n",
    "relation_emb = np.load(r'C:\\Users\\dli0305\\Downloads\\ddis-graph-embeddings\\ddis-graph-embeddings\\relation_embeds.npy')\n",
    "\n",
    "# load the dictionaries\n",
    "with open(r'C:\\Users\\dli0305\\Downloads\\ddis-graph-embeddings\\ddis-graph-embeddings\\entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open(r'C:\\Users\\dli0305\\Downloads\\ddis-graph-embeddings\\ddis-graph-embeddings\\relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(NamespaceWrapper.RDFS.label)}\n",
    "lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
    "\n",
    "# Define your dictionary\n",
    "my_dict = lbl2ent\n",
    "\n",
    "rel2name = {}\n",
    "for _, pred, _ in graph:\n",
    "    label = graph.value(pred, NamespaceWrapper.RDFS.label)\n",
    "    if label:\n",
    "        rel2name[pred] = str(label)\n",
    "    else:\n",
    "        rel2name[pred] = pred.split('/')[-1]\n",
    "\n",
    "rel2name_str = {rdflib.term.URIRef(rel): name for rel, name in rel2name.items()}\n",
    "name2rel = {name: rel for rel, name in rel2name_str.items()}\n",
    "\n",
    "if os.path.exists(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\relationship_bert.pkl\"):\n",
    "    with open(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\relationship_bert.pkl\", \"rb\") as f:\n",
    "        relationship_bert = pickle.load(f)\n",
    "    print(\"Loaded key_embeddings from file.\")\n",
    "else:\n",
    "    relationship_bert = {key: get_embedding(key) for key in name2rel.keys()}\n",
    "    with open(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\relationship_bert.pkl\", \"wb\") as f:\n",
    "        pickle.dump(relationship_bert, f)\n",
    "    print(\"Computed and saved key_embeddings.\")\n",
    "\n",
    "if os.path.exists(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\key_embeddings.pkl\"):\n",
    "    # Load existing key_embeddings from the file\n",
    "    with open(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\key_embeddings.pkl\", \"rb\") as f:\n",
    "        key_embeddings = pickle.load(f)\n",
    "    print(\"Loaded key_embeddings from file.\")\n",
    "else:\n",
    "    # Compute and save key_embeddings if it doesn't exist\n",
    "    #key_embeddings = {key: get_embedding(key) for key in my_dict.keys()}\n",
    "    #with open(r\"C:\\Users\\dli0305\\Desktop\\ATAIChatbot\\ATAIChatbot\\key_embeddings.pkl\", \"wb\") as f:\n",
    "        #pickle.dump(key_embeddings, f)\n",
    "    print(\"Computed and saved key_embeddings.\")\n",
    "\n",
    "# Helper function to get the BERT embedding for a phrase\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Encode dictionary keys\n",
    "#key_embeddings = {key: get_embedding(key) for key in my_dict.keys()}\n",
    "\n",
    "# Function to identify entities using spaCy's NER and match with dictionary keys\n",
    "def find_best_entity_matches(sentence, threshold=0.5):\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Extract potential movie titles by matching dictionary keys with sentence tokens\n",
    "    found_titles = [title for title in key_embeddings.keys() if all(word in words for word in word_tokenize(title))]\n",
    "    longest_title = max(found_titles, key=len) if found_titles else None\n",
    "    \n",
    "    # If we found potential titles, compute similarity\n",
    "    matches = []\n",
    "    if longest_title:\n",
    "        entity_embedding = get_embedding(longest_title)\n",
    "        similarities = {\n",
    "            key: cosine_similarity(entity_embedding, emb.reshape(1, -1)).item()\n",
    "            for key, emb in key_embeddings.items()\n",
    "        }\n",
    "        best_match_key = max(similarities, key=similarities.get)\n",
    "        \n",
    "        # Only consider matches above threshold\n",
    "        if similarities[best_match_key] >= threshold:\n",
    "            matches.append((longest_title, best_match_key, similarities[best_match_key], my_dict[best_match_key]))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def find_best_relationship_matches(sentence, threshold=0.5):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Extract potential movie titles by matching dictionary keys with sentence tokens\n",
    "    found_titles = [title for title in relationship_bert.keys() if all(word in words for word in word_tokenize(title))]\n",
    "    \n",
    "    # If we found potential titles, compute similarity\n",
    "    matches = []\n",
    "    if found_titles:\n",
    "        entity_embedding = get_embedding(found_titles)\n",
    "        similarities = {\n",
    "            key: cosine_similarity(entity_embedding, emb.reshape(1, -1)).item()\n",
    "            for key, emb in relationship_bert.items()\n",
    "        }\n",
    "        best_match_key = max(similarities, key=similarities.get)\n",
    "        \n",
    "        # Only consider matches above threshold\n",
    "        if similarities[best_match_key] >= threshold:\n",
    "            matches.append((found_titles, best_match_key, similarities[best_match_key], name2rel[best_match_key]))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def get_embedding_answer(sentence):\n",
    "    try:\n",
    "        entity_text, entity_matched, entity_similarity, entity_uri = find_best_entity_matches(sentence)[0]\n",
    "        relationship_text, relationship_matched, relationship_similarity, relationship_uri = find_best_relationship_matches(sentence)[0]\n",
    "    except IndexError:\n",
    "        return \"I regret to inform you that there is no answer available at this time.\"\n",
    "\n",
    "    \n",
    "    if entity_uri != None or relationship_uri != None:\n",
    "        head = entity_emb[ent2id[NamespaceWrapper.uri_to_prefixed(entity_uri)]]\n",
    "        pred = relation_emb[rel2id[NamespaceWrapper.uri_to_prefixed(relationship_uri)]]\n",
    "\n",
    "        lhs = head + pred\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        most_likely = dist.argsort()\n",
    "        entity_uri = id2ent[most_likely[0]]\n",
    "        label = ent2lbl.get(entity_uri, \"I regret to inform you that there is no answer available at this time.\")\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I regret to inform you that there is no answer available at this time.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_answer('When was \"The Godfather\" released? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity URI: http://www.wikidata.org/entity/Q7750525\n",
      "Relationship URI: http://www.wikidata.org/prop/direct/P58\n",
      "SPARQL Query:\n",
      "\n",
      "SELECT ?ans WHERE { \n",
      "    ?a rdfs:label <http://www.wikidata.org/entity/Q7750525>.  \n",
      "    ?a <http://www.wikidata.org/prop/direct/P58> ?b . \n",
      "    ?b rdfs:label ?ans . \n",
      "}\n",
      "LIMIT 1\n",
      "\n",
      "No answer found.\n"
     ]
    }
   ],
   "source": [
    "# Extract entity and relationship matches\n",
    "entity_text, entity_matched, entity_similarity, entity_uri = find_best_entity_matches(\n",
    "    'Who is the screenwriter of The Masked Gang: Cyprus? ')[0]\n",
    "relationship_text, relationship_matched, relationship_similarity, relationship_uri = find_best_relationship_matches(\n",
    "    'Who is the screenwriter of The Masked Gang: Cyprus? ')[0]\n",
    "\n",
    "# Print debug information\n",
    "print(\"Entity URI:\", entity_uri)\n",
    "print(\"Relationship URI:\", relationship_uri)\n",
    "\n",
    "# Generate SPARQL query\n",
    "query = f\"\"\"\n",
    "SELECT ?ans WHERE {{ \n",
    "    ?a rdfs:label <{NamespaceWrapper.uri_to_prefixed(entity_uri)}>.  \n",
    "    ?a <{NamespaceWrapper.uri_to_prefixed(relationship_uri)}> ?b . \n",
    "    ?b rdfs:label ?ans . \n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Print query for debugging\n",
    "print(\"SPARQL Query:\")\n",
    "print(query)\n",
    "\n",
    "# Run the query\n",
    "try:\n",
    "    results = graph.query(query)\n",
    "    # Return first answer or default response if none found\n",
    "    found_answer = False\n",
    "    for row in results:\n",
    "        # Convert the answer URI to string and extract the final part\n",
    "        answer = str(row[0]) if isinstance(row[0], URIRef) else row[0]\n",
    "        print(f\"I think it is {answer.split('/')[-1]}.\")\n",
    "        found_answer = True\n",
    "    if not found_answer:\n",
    "        print(\"No answer found.\")\n",
    "except Exception as e:\n",
    "    print(\"Error running query:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
